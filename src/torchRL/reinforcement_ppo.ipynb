{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (PPO) with TorchRL Tutorial\n",
    "==================================================\n",
    "\n",
    "**Author**: [Vincent Moens](https://github.com/vmoens)\n",
    "\n",
    "This tutorial demonstrates how to use PyTorch and\n",
    ":py`torchrl`{.interpreted-text role=\"mod\"} to train a parametric policy\n",
    "network to solve the Inverted Pendulum task from the\n",
    "[OpenAI-Gym/Farama-Gymnasium control\n",
    "library](https://github.com/Farama-Foundation/Gymnasium).\n",
    "\n",
    "![Inverted\n",
    "pendulum](https://pytorch.org/tutorials/_static/img/invpendulum.gif)\n",
    "\n",
    "Key learnings:\n",
    "\n",
    "-   How to create an environment in TorchRL, transform its outputs, and\n",
    "    collect data from this environment;\n",
    "-   How to make your classes talk to each other using\n",
    "    `~tensordict.TensorDict`{.interpreted-text role=\"class\"};\n",
    "-   The basics of building your training loop with TorchRL:\n",
    "    -   How to compute the advantage signal for policy gradient methods;\n",
    "    -   How to create a stochastic policy using a probabilistic neural\n",
    "        network;\n",
    "    -   How to create a dynamic replay buffer and sample from it without\n",
    "        repetition.\n",
    "\n",
    "We will cover six crucial components of TorchRL:\n",
    "\n",
    "-   [environments](https://pytorch.org/rl/reference/envs.html)\n",
    "-   [transforms](https://pytorch.org/rl/reference/envs.html#transforms)\n",
    "-   [models (policy and value\n",
    "    function)](https://pytorch.org/rl/reference/modules.html)\n",
    "-   [loss modules](https://pytorch.org/rl/reference/objectives.html)\n",
    "-   [data collectors](https://pytorch.org/rl/reference/collectors.html)\n",
    "-   [replay\n",
    "    buffers](https://pytorch.org/rl/reference/data.html#replay-buffers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this in Google Colab, make sure you install the\n",
    "following dependencies:\n",
    "\n",
    "``` {.sourceCode .bash}\n",
    "!pip3 install torchrl\n",
    "!pip3 install gym[mujoco]\n",
    "!pip3 install tqdm\n",
    "```\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a policy-gradient algorithm where\n",
    "a batch of data is being collected and directly consumed to train the\n",
    "policy to maximise the expected return given some proximality\n",
    "constraints. You can think of it as a sophisticated version of\n",
    "[REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf),\n",
    "the foundational policy-optimization algorithm. For more information,\n",
    "see the [Proximal Policy Optimization\n",
    "Algorithms](https://arxiv.org/abs/1707.06347) paper.\n",
    "\n",
    "PPO is usually regarded as a fast and efficient method for online,\n",
    "on-policy reinforcement algorithm. TorchRL provides a loss-module that\n",
    "does all the work for you, so that you can rely on this implementation\n",
    "and focus on solving your problem rather than re-inventing the wheel\n",
    "every time you want to train a policy.\n",
    "\n",
    "For completeness, here is a brief overview of what the loss computes,\n",
    "even though this is taken care of by our\n",
    "`~torchrl.objectives.ClipPPOLoss`{.interpreted-text role=\"class\"}\n",
    "module---the algorithm works as follows: 1. we will sample a batch of\n",
    "data by playing the policy in the environment for a given number of\n",
    "steps. 2. Then, we will perform a given number of optimization steps\n",
    "with random sub-samples of this batch using a clipped version of the\n",
    "REINFORCE loss. 3. The clipping will put a pessimistic bound on our\n",
    "loss: lower return estimates will be favored compared to higher ones.\n",
    "The precise formula of the loss is:\n",
    "\n",
    "$$L(s,a,\\theta_k,\\theta) = \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n",
    "g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n",
    "\\right),$$\n",
    "\n",
    "There are two components in that loss: in the first part of the minimum\n",
    "operator, we simply compute an importance-weighted version of the\n",
    "REINFORCE loss (for example, a REINFORCE loss that we have corrected for\n",
    "the fact that the current policy configuration lags the one that was\n",
    "used for the data collection). The second part of that minimum operator\n",
    "is a similar loss where we have clipped the ratios when they exceeded or\n",
    "were below a given pair of thresholds.\n",
    "\n",
    "This loss ensures that whether the advantage is positive or negative,\n",
    "policy updates that would produce significant shifts from the previous\n",
    "configuration are being discouraged.\n",
    "\n",
    "This tutorial is structured as follows:\n",
    "\n",
    "1.  First, we will define a set of hyperparameters we will be using for\n",
    "    training.\n",
    "2.  Next, we will focus on creating our environment, or simulator, using\n",
    "    TorchRL\\'s wrappers and transforms.\n",
    "3.  Next, we will design the policy network and the value model, which\n",
    "    is indispensable to the loss function. These modules will be used to\n",
    "    configure our loss module.\n",
    "4.  Next, we will create the replay buffer and data loader.\n",
    "5.  Finally, we will run our training loop and analyze the results.\n",
    "\n",
    "Throughout this tutorial, we\\'ll be using the\n",
    "`tensordict`{.interpreted-text role=\"mod\"} library.\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} is the lingua\n",
    "franca of TorchRL: it helps us abstract what a module reads and writes\n",
    "and care less about the specific data description and more about the\n",
    "algorithm itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n",
    "                          TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Hyperparameters\n",
    "======================\n",
    "\n",
    "We set the hyperparameters for our algorithm. Depending on the resources\n",
    "available, one may choose to execute the policy on GPU or on another\n",
    "device. The `frame_skip` will control how for how many frames is a\n",
    "single action being executed. The rest of the arguments that count\n",
    "frames must be corrected for this value (since one environment step will\n",
    "actually return `frame_skip` frames).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection parameters\n",
    "==========================\n",
    "\n",
    "When collecting data, we will be able to choose how big each batch will\n",
    "be by defining a `frames_per_batch` parameter. We will also define how\n",
    "many frames (such as the number of interactions with the simulator) we\n",
    "will allow ourselves to use. In general, the goal of an RL algorithm is\n",
    "to learn to solve the task as fast as it can in terms of environment\n",
    "interactions: the lower the `total_frames` the better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO parameters\n",
    "==============\n",
    "\n",
    "At each data collection (or batch collection) we will run the\n",
    "optimization over a certain number of *epochs*, each time consuming the\n",
    "entire data we just acquired in a nested training loop. Here, the\n",
    "`sub_batch_size` is different from the `frames_per_batch` here above:\n",
    "recall that we are working with a \\\"batch of data\\\" coming from our\n",
    "collector, which size is defined by `frames_per_batch`, and that we will\n",
    "further split in smaller sub-batches during the inner training loop. The\n",
    "size of these sub-batches is controlled by `sub_batch_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an environment\n",
    "=====================\n",
    "\n",
    "In RL, an *environment* is usually the way we refer to a simulator or a\n",
    "control system. Various libraries provide simulation environments for\n",
    "reinforcement learning, including Gymnasium (previously OpenAI Gym),\n",
    "DeepMind control suite, and many others. As a general library,\n",
    "TorchRL\\'s goal is to provide an interchangeable interface to a large\n",
    "panel of RL simulators, allowing you to easily swap one environment with\n",
    "another. For example, creating a wrapped gym environment can be achieved\n",
    "with few characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MultiAgentWrapper' object is not callable was raised from the environment creator for intersection-multi-agent-v1 with kwargs ({'render_mode': 'rgb_array'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/envs/registration.py:802\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 802\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_spec_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MultiAgentWrapper' object is not callable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GymWrapper\n\u001b[0;32m----> 5\u001b[0m base_env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintersection-multi-agent-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Wrap the environment\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/envs/registration.py:814\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    809\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed render_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m although \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_spec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt implement human-rendering natively. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym tried to apply the HumanRendering wrapper but it looks like your environment is using the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering API, which is not supported by the HumanRendering wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was raised from the environment creator for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_spec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_spec_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         )\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# Set the minimal env spec for the environment.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mspec \u001b[38;5;241m=\u001b[39m EnvSpec(\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39menv_spec\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    821\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39menv_spec\u001b[38;5;241m.\u001b[39mentry_point,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    831\u001b[0m     vector_entry_point\u001b[38;5;241m=\u001b[39menv_spec\u001b[38;5;241m.\u001b[39mvector_entry_point,\n\u001b[1;32m    832\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MultiAgentWrapper' object is not callable was raised from the environment creator for intersection-multi-agent-v1 with kwargs ({'render_mode': 'rgb_array'})"
     ]
    }
   ],
   "source": [
    "# base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)\n",
    "import gymnasium as gym\n",
    "from torchrl.envs import GymWrapper\n",
    "\n",
    "base_env = gym.make(\"intersection-multi-agent-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Wrap the environment\n",
    "base_env = GymWrapper(base_env, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': {'type': 'MultiAgentObservation',\n",
       "  'observation_config': {'type': 'Kinematics'}},\n",
       " 'action': {'type': 'MultiAgentAction',\n",
       "  'action_config': {'type': 'DiscreteMetaAction',\n",
       "   'lateral': False,\n",
       "   'longitudinal': True}},\n",
       " 'simulation_frequency': 15,\n",
       " 'policy_frequency': 1,\n",
       " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
       " 'screen_width': 600,\n",
       " 'screen_height': 600,\n",
       " 'centering_position': [0.5, 0.6],\n",
       " 'scaling': 7.15,\n",
       " 'show_trajectories': False,\n",
       " 'render_agent': True,\n",
       " 'offscreen_rendering': False,\n",
       " 'manual_control': False,\n",
       " 'real_time_rendering': False,\n",
       " 'duration': 13,\n",
       " 'destination': 'o1',\n",
       " 'controlled_vehicles': 2,\n",
       " 'initial_vehicle_count': 10,\n",
       " 'spawn_probability': 0.6,\n",
       " 'collision_reward': -5,\n",
       " 'high_speed_reward': 1,\n",
       " 'arrived_reward': 1,\n",
       " 'reward_speed_range': [7.0, 9.0],\n",
       " 'normalize_reward': False,\n",
       " 'offroad_terminal': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotDiscreteTensorSpec(\n",
       "    shape=torch.Size([2, 3]),\n",
       "    space=DiscreteBox(n=3),\n",
       "    device=mps,\n",
       "    dtype=torch.int64,\n",
       "    domain=discrete)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(3), Discrete(3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnboundedContinuousTensorSpec(\n",
       "    shape=torch.Size([2, 5, 5]),\n",
       "    space=None,\n",
       "    device=mps,\n",
       "    dtype=torch.float32,\n",
       "    domain=continuous)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.observation_spec[\"observation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to notice in this code: first, we created the\n",
    "environment by calling the `GymEnv` wrapper. If extra keyword arguments\n",
    "are passed, they will be transmitted to the `gym.make` method, hence\n",
    "covering the most common environment construction commands.\n",
    "Alternatively, one could also directly create a gym environment using\n",
    "`gym.make(env_name, **kwargs)` and wrap it in a [GymWrapper]{.title-ref}\n",
    "class.\n",
    "\n",
    "Also the `device` argument: for gym, this only controls the device where\n",
    "input action and observed states will be stored, but the execution will\n",
    "always be done on CPU. The reason for this is simply that gym does not\n",
    "support on-device execution, unless specified otherwise. For other\n",
    "libraries, we have control over the execution device and, as much as we\n",
    "can, we try to stay consistent in terms of storing and execution\n",
    "backends.\n",
    "\n",
    "Transforms\n",
    "==========\n",
    "\n",
    "We will append some transforms to our environments to prepare the data\n",
    "for the policy. In Gym, this is usually achieved via wrappers. TorchRL\n",
    "takes a different approach, more similar to other pytorch domain\n",
    "libraries, through the use of transforms. To add transforms to an\n",
    "environment, one should simply wrap it in a\n",
    "`~torchrl.envs.transforms.TransformedEnv`{.interpreted-text\n",
    "role=\"class\"} instance and append the sequence of transforms to it. The\n",
    "transformed environment will inherit the device and meta-data of the\n",
    "wrapped environment, and transform these depending on the sequence of\n",
    "transforms it contains.\n",
    "\n",
    "Normalization\n",
    "=============\n",
    "\n",
    "The first to encode is a normalization transform. As a rule of thumbs,\n",
    "it is preferable to have data that loosely match a unit Gaussian\n",
    "distribution: to obtain this, we will run a certain number of random\n",
    "steps in the environment and compute the summary statistics of these\n",
    "observations.\n",
    "\n",
    "We\\'ll append two other transforms: the\n",
    "`~torchrl.envs.transforms.DoubleToFloat`{.interpreted-text role=\"class\"}\n",
    "transform will convert double entries to single-precision numbers, ready\n",
    "to be read by the policy. The\n",
    "`~torchrl.envs.transforms.StepCounter`{.interpreted-text role=\"class\"}\n",
    "transform will be used to count the steps before the environment is\n",
    "terminated. We will use this measure as a supplementary measure of\n",
    "performance.\n",
    "\n",
    "As we will see later, many of the TorchRL\\'s classes rely on\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} to communicate.\n",
    "You could think of it as a python dictionary with some extra tensor\n",
    "features. In practice, this means that many modules we will be working\n",
    "with need to be told what key to read (`in_keys`) and what key to write\n",
    "(`out_keys`) in the `tensordict` they will receive. Usually, if\n",
    "`out_keys` is omitted, it is assumed that the `in_keys` entries will be\n",
    "updated in-place. For our transforms, the only entry we are interested\n",
    "in is referred to as `\"observation\"` and our transform layers will be\n",
    "told to modify this entry and this entry only:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, we have created a normalization layer but we\n",
    "did not set its normalization parameters. To do this,\n",
    "`~torchrl.envs.transforms.ObservationNorm`{.interpreted-text\n",
    "role=\"class\"} can automatically gather the summary statistics of our\n",
    "environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/transforms/transforms.py:2609\u001b[0m, in \u001b[0;36mObservationNorm.init_stats\u001b[0;34m(self, num_iter, reduce_dim, cat_dim, key, keep_dims)\u001b[0m\n\u001b[1;32m   2607\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2608\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m collected_frames \u001b[38;5;241m<\u001b[39m num_iter:\n\u001b[0;32m-> 2609\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2610\u001b[0m     collected_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m   2611\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(tensordict\u001b[38;5;241m.\u001b[39mget(key))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:2562\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, set_truncated, out)\u001b[0m\n\u001b[1;32m   2552\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensordict\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensordict,\n\u001b[1;32m   2554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_cast_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m: auto_cast_to_device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: callback,\n\u001b[1;32m   2560\u001b[0m }\n\u001b[1;32m   2561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done:\n\u001b[0;32m-> 2562\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollout_stop_early\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2564\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:2638\u001b[0m, in \u001b[0;36mEnvBase._rollout_stop_early\u001b[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[1;32m   2636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2637\u001b[0m         tensordict\u001b[38;5;241m.\u001b[39mclear_device_()\n\u001b[0;32m-> 2638\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2639\u001b[0m td_append \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   2640\u001b[0m tensordicts\u001b[38;5;241m.\u001b[39mappend(td_append)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:1461\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m   1459\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1461\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/transforms/transforms.py:779\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    777\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    778\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[0;32m--> 779\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    785\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/gym_like.py:294\u001b[0m, in \u001b[0;36mGymLikeEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    285\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_frame_skip):\n\u001b[1;32m    287\u001b[0m     (\n\u001b[1;32m    288\u001b[0m         obs,\n\u001b[1;32m    289\u001b[0m         _reward,\n\u001b[1;32m    290\u001b[0m         terminated,\n\u001b[1;32m    291\u001b[0m         truncated,\n\u001b[1;32m    292\u001b[0m         done,\n\u001b[1;32m    293\u001b[0m         info_dict,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_np\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m         reward \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m _reward\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/intersection_env.py:123\u001b[0m, in \u001b[0;36mIntersectionEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m--> 123\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_vehicles()\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn_vehicle(spawn_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:255\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frames):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# Forward action to the vehicle\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[1;32m    254\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/action.py:290\u001b[0m, in \u001b[0;36mMultiAgentAction.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: Action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_action, action_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_action_types):\n\u001b[1;32m    292\u001b[0m         action_type\u001b[38;5;241m.\u001b[39mact(agent_action)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.transform[0].init_stats(num_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `~torchrl.envs.transforms.ObservationNorm`{.interpreted-text\n",
    "role=\"class\"} transform has now been populated with a location and a\n",
    "scale that will be used to normalize the data.\n",
    "\n",
    "Let us do a little sanity check for the shape of our summary stats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalization constant shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torch/nn/parameter.py:129\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt access the shape of an uninitialized parameter or buffer. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error usually happens in `load_state_dict` when trying to load \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man uninitialized parameter into an initialized one. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCall `forward` to initialize the parameters before accessing their attributes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't access the shape of an uninitialized parameter or buffer. This error usually happens in `load_state_dict` when trying to load an uninitialized parameter into an initialized one. Call `forward` to initialize the parameters before accessing their attributes."
     ]
    }
   ],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An environment is not only defined by its simulator and transforms, but\n",
    "also by a series of metadata that describe what can be expected during\n",
    "its execution. For efficiency purposes, TorchRL is quite stringent when\n",
    "it comes to environment specs, but you can easily check that your\n",
    "environment specs are adequate. In our example, the\n",
    "`~torchrl.envs.libs.gym.GymWrapper`{.interpreted-text role=\"class\"} and\n",
    "`~torchrl.envs.libs.gym.GymEnv`{.interpreted-text role=\"class\"} that\n",
    "inherits from it already take care of setting the proper specs for your\n",
    "environment so you should not have to care about this.\n",
    "\n",
    "Nevertheless, let\\'s see a concrete example using our transformed\n",
    "environment by looking at its specs. There are three specs to look at:\n",
    "`observation_spec` which defines what is to be expected when executing\n",
    "an action in the environment, `reward_spec` which indicates the reward\n",
    "domain and finally the `input_spec` (which contains the `action_spec`)\n",
    "and which represents everything an environment requires to execute a\n",
    "single step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    observation: UnboundedContinuousTensorSpec(\n",
      "        shape=torch.Size([5, 8]),\n",
      "        space=None,\n",
      "        device=mps,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    step_count: BoundedTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.int64, contiguous=True)),\n",
      "        device=mps,\n",
      "        dtype=torch.int64,\n",
      "        domain=continuous), device=mps, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=mps,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n",
      "input_spec: CompositeSpec(\n",
      "    full_state_spec: CompositeSpec(\n",
      "        step_count: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.int64, contiguous=True)),\n",
      "            device=mps,\n",
      "            dtype=torch.int64,\n",
      "            domain=continuous), device=mps, shape=torch.Size([])),\n",
      "    full_action_spec: CompositeSpec(\n",
      "        action: BoundedTensorSpec(\n",
      "            shape=torch.Size([2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([2]), device=mps:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([2]), device=mps:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=mps,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=mps, shape=torch.Size([])), device=mps, shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): BoundedTensorSpec(\n",
      "    shape=torch.Size([2]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([2]), device=mps:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([2]), device=mps:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=mps,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `check_env_specs`{.interpreted-text role=\"func\"} function runs a\n",
    "small rollout and compares its output against the environment specs. If\n",
    "no error is raised, we can be confident that the specs are properly\n",
    "defined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 18:07:46,428 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let\\'s see what a simple random rollout looks like. You can\n",
    "call [env.rollout(n\\_steps)]{.title-ref} and get an overview of what the\n",
    "environment inputs and outputs look like. Actions will automatically be\n",
    "drawn from the action spec domain, so you don\\'t need to care about\n",
    "designing a random sampler.\n",
    "\n",
    "Typically, at each step, an RL environment receives an action as input,\n",
    "and outputs an observation, a reward and a done state. The observation\n",
    "may be composite, meaning that it could be composed of more than one\n",
    "tensor. This is not a problem for TorchRL, since the whole set of\n",
    "observations is automatically packed in the output\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"}. After\n",
    "executing a rollout (for example, a sequence of environment steps and\n",
    "random action generations) over a given number of steps, we will\n",
    "retrieve a `~tensordict.TensorDict`{.interpreted-text role=\"class\"}\n",
    "instance with a shape that matches this trajectory length:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1, 2]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([1, 5, 8]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=mps,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([1, 5, 8]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=mps,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rollout data has a shape of `torch.Size([3])`, which matches the\n",
    "number of steps we ran it for. The `\"next\"` entry points to the data\n",
    "coming after the current step. In most cases, the `\"next\"` data at time\n",
    "[t]{.title-ref} matches the data at `t+1`, but this may not be the case\n",
    "if we are using some specific transformations (for example, multi-step).\n",
    "\n",
    "Policy\n",
    "======\n",
    "\n",
    "PPO utilizes a stochastic policy to handle exploration. This means that\n",
    "our neural network will have to output the parameters of a distribution,\n",
    "rather than a single value corresponding to the action taken.\n",
    "\n",
    "As the data is continuous, we use a Tanh-Normal distribution to respect\n",
    "the action space boundaries. TorchRL provides such distribution, and the\n",
    "only thing we need to care about is to build a neural network that\n",
    "outputs the right number of parameters for the policy to work with (a\n",
    "location, or mean, and a scale):\n",
    "\n",
    "$$f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})$$\n",
    "\n",
    "The only extra-difficulty that is brought up here is to split our output\n",
    "in two equal parts and map the second to a strictly positive space.\n",
    "\n",
    "We design the policy in three steps:\n",
    "\n",
    "1.  Define a neural network `D_obs` -\\> `2 * D_action`. Indeed, our\n",
    "    `loc` (mu) and `scale` (sigma) both have dimension `D_action`.\n",
    "2.  Append a\n",
    "    `~tensordict.nn.distributions.NormalParamExtractor`{.interpreted-text\n",
    "    role=\"class\"} to extract a location and a scale (for example, splits\n",
    "    the input in two equal parts and applies a positive transformation\n",
    "    to the scale parameter).\n",
    "3.  Create a probabilistic\n",
    "    `~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"}\n",
    "    that can generate this distribution and sample from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_actions = env.action_spec.shape[-1]\n",
    "d_obs = env.observation_spec[\"observation\"].shape[-1]\n",
    "\n",
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(d_obs, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable the policy to \\\"talk\\\" with the environment through the\n",
    "`tensordict` data carrier, we wrap the `nn.Module` in a\n",
    "`~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"}. This\n",
    "class will simply ready the `in_keys` it is provided with and write the\n",
    "outputs in-place at the registered `out_keys`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to build a distribution out of the location and scale of our\n",
    "normal distribution. To do so, we instruct the\n",
    "`~torchrl.modules.tensordict_module.ProbabilisticActor`{.interpreted-text\n",
    "role=\"class\"} class to build a\n",
    "`~torchrl.modules.TanhNormal`{.interpreted-text role=\"class\"} out of the\n",
    "location and scale parameters. We also provide the minimum and maximum\n",
    "values of this distribution, which we gather from the environment specs.\n",
    "\n",
    "The name of the `in_keys` (and hence the name of the `out_keys` from the\n",
    "`~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"} above)\n",
    "cannot be set to any value one may like, as the\n",
    "`~torchrl.modules.TanhNormal`{.interpreted-text role=\"class\"}\n",
    "distribution constructor will expect the `loc` and `scale` keyword\n",
    "arguments. That being said,\n",
    "`~torchrl.modules.tensordict_module.ProbabilisticActor`{.interpreted-text\n",
    "role=\"class\"} also accepts `Dict[str, str]` typed `in_keys` where the\n",
    "key-value pair indicates what `in_key` string should be used for every\n",
    "keyword argument that is to be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.low,\n",
    "        \"max\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value network\n",
    "=============\n",
    "\n",
    "The value network is a crucial component of the PPO algorithm, even\n",
    "though it won\\'t be used at inference time. This module will read the\n",
    "observations and return an estimation of the discounted return for the\n",
    "following trajectory. This allows us to amortize learning by relying on\n",
    "the some utility estimation that is learned on-the-fly during training.\n",
    "Our value network share the same structure as the policy, but for\n",
    "simplicity we assign it its own set of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let\\'s try our policy and value modules. As we said earlier, the usage\n",
    "of `~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"}\n",
    "makes it possible to directly read the output of the environment to run\n",
    "these modules, as they know what information to read and where to write\n",
    "it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 2]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([5, 2]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 8]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([5]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([5, 2]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=mps,\n",
      "    is_shared=False)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([5, 8]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        state_value: Tensor(shape=torch.Size([5, 1]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=mps:0, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=mps,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collector\n",
    "==============\n",
    "\n",
    "TorchRL provides a set of [DataCollector\n",
    "classes](https://pytorch.org/rl/reference/collectors.html). Briefly,\n",
    "these classes execute three operations: reset an environment, compute an\n",
    "action given the latest observation, execute a step in the environment,\n",
    "and repeat the last two steps until the environment signals a stop (or\n",
    "reaches a done state).\n",
    "\n",
    "They allow you to control how many frames to collect at each iteration\n",
    "(through the `frames_per_batch` parameter), when to reset the\n",
    "environment (through the `max_frames_per_traj` argument), on which\n",
    "`device` the policy should be executed, etc. They are also designed to\n",
    "work efficiently with batched and multiprocessed environments.\n",
    "\n",
    "The simplest data collector is the\n",
    "`~torchrl.collectors.collectors.SyncDataCollector`{.interpreted-text\n",
    "role=\"class\"}: it is an iterator that you can use to get batches of data\n",
    "of a given length, and that will stop once a total number of frames\n",
    "(`total_frames`) have been collected. Other data collectors\n",
    "(`~torchrl.collectors.collectors.MultiSyncDataCollector`{.interpreted-text\n",
    "role=\"class\"} and\n",
    "`~torchrl.collectors.collectors.MultiaSyncDataCollector`{.interpreted-text\n",
    "role=\"class\"}) will execute the same operations in synchronous and\n",
    "asynchronous manner over a set of multiprocessed workers.\n",
    "\n",
    "As for the policy and environment before, the data collector will return\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} instances with\n",
    "a total number of elements that will match `frames_per_batch`. Using\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} to pass data to\n",
    "the training loop allows you to write data loading pipelines that are\n",
    "100% oblivious to the actual specificities of the rollout content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay buffer\n",
    "=============\n",
    "\n",
    "Replay buffers are a common building piece of off-policy RL algorithms.\n",
    "In on-policy contexts, a replay buffer is refilled every time a batch of\n",
    "data is collected, and its data is repeatedly consumed for a certain\n",
    "number of epochs.\n",
    "\n",
    "TorchRL\\'s replay buffers are built using a common container\n",
    "`~torchrl.data.ReplayBuffer`{.interpreted-text role=\"class\"} which takes\n",
    "as argument the components of the buffer: a storage, a writer, a sampler\n",
    "and possibly some transforms. Only the storage (which indicates the\n",
    "replay buffer capacity) is mandatory. We also specify a sampler without\n",
    "repetition to avoid sampling multiple times the same item in one epoch.\n",
    "Using a replay buffer for PPO is not mandatory and we could simply\n",
    "sample the sub-batches from the collected batch, but using these classes\n",
    "make it easy for us to build the inner training loop in a reproducible\n",
    "way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n",
    "=============\n",
    "\n",
    "The PPO loss can be directly imported from TorchRL for convenience using\n",
    "the `~torchrl.objectives.ClipPPOLoss`{.interpreted-text role=\"class\"}\n",
    "class. This is the easiest way of utilizing PPO: it hides away the\n",
    "mathematical operations of PPO and the control flow that goes with it.\n",
    "\n",
    "PPO requires some \\\"advantage estimation\\\" to be computed. In short, an\n",
    "advantage is a value that reflects an expectancy over the return value\n",
    "while dealing with the bias / variance tradeoff. To compute the\n",
    "advantage, one just needs to (1) build the advantage module, which\n",
    "utilizes our value operator, and (2) pass each batch of data through it\n",
    "before each epoch. The GAE module will update the input `tensordict`\n",
    "with new `\"advantage\"` and `\"value_target\"` entries. The\n",
    "`\"value_target\"` is a gradient-free tensor that represents the empirical\n",
    "value that the value network should represent with the input\n",
    "observation. Both of these will be used by\n",
    "`~torchrl.objectives.ClipPPOLoss`{.interpreted-text role=\"class\"} to\n",
    "return the policy and value losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "=============\n",
    "\n",
    "We now have all the pieces needed to code our training loop. The steps\n",
    "include:\n",
    "\n",
    "-   Collect data\n",
    "    -   Compute advantage\n",
    "        -   Loop over the collected to compute loss values\n",
    "        -   Back propagate\n",
    "        -   Optimize\n",
    "        -   Repeat\n",
    "    -   Repeat\n",
    "-   Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of one step: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1, 2]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([1, 5, 8]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=mps,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([1, 5, 8]), device=mps:0, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1, 1]), device=mps:0, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=mps,\n",
      "    is_shared=False)\n",
      "acceleration action: tensor([[ 0.5315, -0.3917]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# print value of acceleration action after one rollout\n",
    "rollout = env.rollout(1)\n",
    "print(\"rollout of one step:\", rollout)\n",
    "print(\"acceleration action:\", rollout[\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/_utils.py:480\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/collectors/collectors.py:1007\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[0;32m-> 1007\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:2746\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[1;32m   2706\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \n\u001b[1;32m   2709\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2746\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2747\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_mdp(tensordict)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:1461\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m   1459\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1461\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/transforms/transforms.py:779\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    777\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    778\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[0;32m--> 779\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    785\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/gym_like.py:294\u001b[0m, in \u001b[0;36mGymLikeEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    285\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_frame_skip):\n\u001b[1;32m    287\u001b[0m     (\n\u001b[1;32m    288\u001b[0m         obs,\n\u001b[1;32m    289\u001b[0m         _reward,\n\u001b[1;32m    290\u001b[0m         terminated,\n\u001b[1;32m    291\u001b[0m         truncated,\n\u001b[1;32m    292\u001b[0m         done,\n\u001b[1;32m    293\u001b[0m         info_dict,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_np\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m         reward \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m _reward\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/intersection_env.py:123\u001b[0m, in \u001b[0;36mIntersectionEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m--> 123\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_vehicles()\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn_vehicle(spawn_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/road/regulation.py:25\u001b[0m, in \u001b[0;36mRegulatedRoad.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m dt \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREGULATION_FREQUENCY) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_road_rules()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/road/road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/vehicle/dynamics.py:116\u001b[0m, in \u001b[0;36mBicycleVehicle.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m rk4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dt\u001b[38;5;241m=\u001b[39mdt)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m new_state[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/vehicle/dynamics.py:127\u001b[0m, in \u001b[0;36mBicycleVehicle.clip_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Required because of the linearisation\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/vehicle/kinematics.py:139\u001b[0m, in \u001b[0;36mVehicle.clip_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_SPEED:\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "collector.rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [30:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m eval_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# We iterate over the collector until it reaches the total number of frames it was\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# designed to collect:\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we now have a batch of data to work with. Let's learn something from it.\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# We'll need an \"advantage\" signal to make PPO work.\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# We re-compute it at each epoch as its value depends on the value\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# network which is updated in the inner loop.\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvantage_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/collectors/collectors.py:888\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_frames:\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 888\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tensordict_out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_frames:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/_utils.py:480\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/collectors/collectors.py:1007\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[0;32m-> 1007\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:2746\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[1;32m   2706\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \n\u001b[1;32m   2709\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2746\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2747\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_mdp(tensordict)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/common.py:1461\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m   1459\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1461\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/transforms/transforms.py:779\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    777\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    778\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[0;32m--> 779\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    785\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/torchrl/envs/gym_like.py:294\u001b[0m, in \u001b[0;36mGymLikeEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    285\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_frame_skip):\n\u001b[1;32m    287\u001b[0m     (\n\u001b[1;32m    288\u001b[0m         obs,\n\u001b[1;32m    289\u001b[0m         _reward,\n\u001b[1;32m    290\u001b[0m         terminated,\n\u001b[1;32m    291\u001b[0m         truncated,\n\u001b[1;32m    292\u001b[0m         done,\n\u001b[1;32m    293\u001b[0m         info_dict,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_np\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m         reward \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m _reward\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/intersection_env.py:123\u001b[0m, in \u001b[0;36mIntersectionEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m--> 123\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_vehicles()\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn_vehicle(spawn_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/road/regulation.py:25\u001b[0m, in \u001b[0;36mRegulatedRoad.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m dt \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREGULATION_FREQUENCY) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_road_rules()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/road/road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/vehicle/dynamics.py:116\u001b[0m, in \u001b[0;36mBicycleVehicle.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m rk4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dt\u001b[38;5;241m=\u001b[39mdt)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m new_state[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/vehicle/dynamics.py:127\u001b[0m, in \u001b[0;36mBicycleVehicle.clip_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Required because of the linearisation\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/MARS-Traffic-KmMWCuv8/lib/python3.11/site-packages/highway_env/vehicle/kinematics.py:139\u001b[0m, in \u001b[0;36mVehicle.clip_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_SPEED:\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "=======\n",
    "\n",
    "Before the 1M step cap is reached, the algorithm should have reached a\n",
    "max step count of 1000 steps, which is the maximum number of steps\n",
    "before the trajectory is truncated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion and next steps\n",
    "=========================\n",
    "\n",
    "In this tutorial, we have learned:\n",
    "\n",
    "1.  How to create and customize an environment with\n",
    "    :py`torchrl`{.interpreted-text role=\"mod\"};\n",
    "2.  How to write a model and a loss function;\n",
    "3.  How to set up a typical training loop.\n",
    "\n",
    "If you want to experiment with this tutorial a bit more, you can apply\n",
    "the following modifications:\n",
    "\n",
    "-   From an efficiency perspective, we could run several simulations in\n",
    "    parallel to speed up data collection. Check\n",
    "    `~torchrl.envs.ParallelEnv`{.interpreted-text role=\"class\"} for\n",
    "    further information.\n",
    "-   From a logging perspective, one could add a\n",
    "    `torchrl.record.VideoRecorder`{.interpreted-text role=\"class\"}\n",
    "    transform to the environment after asking for rendering to get a\n",
    "    visual rendering of the inverted pendulum in action. Check\n",
    "    :py`torchrl.record`{.interpreted-text role=\"mod\"} to know more.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
